<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Zishun Yu"><title>Self-critical Sequence Training for Image Captioning with SPICE · Zishun's Blog</title><meta name="description" content="This is a project of interset and topic of this project is Self-critical Sequence Training for image captioning with SPICE. In this study, I was inspi"><meta name="keywords" content><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/bootstrap.min.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/style.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML" async></script><link rel="stylesheet" href="/css/prism.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div id="stage" class="container"><div class="row"><div id="side-bar" class="col-sm-3 col-xs-12 side-container invisible"><div class="vertical-text site-title"><h3 tabindex="-1" class="site-title-small"><a href="/" class="a-title">Perception</a></h3><h1 tabindex="-1" class="site-title-large"><a href="/" class="a-title">認識能力</a></h1><!--h6(onclick="triggerSiteNav()") Trigger--></div><br class="visible-lg visible-md visible-sm"><div id="site-nav" class="site-title-links"><ul><li><a href="/">Home</a></li><li></li><li><a href="/404.html"></a></li><li><a href="/index.html"></a></li><li><a href="/Blog/index.html" class="current">Blog</a></li><li><a href="/CV/index.html">CV</a></li><li class="soc"><a href="https://github.com/ZishunYu" target="_blank" rel="noopener noreferrer"><i class="fa fa-github">&nbsp;</i></a></li></ul><div class="visible-lg visible-md visible-sm site-nav-footer"><br class="site-nav-footer-br"><footer><p>&copy;&nbsp;2019&nbsp;<a target="_blank" href="http://www.zishun.xyz" rel="noopener noreferrer">Zishun Yu</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div><div id="main-container" class="col-sm-9 col-xs-12 main-container invisible"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post-container"><p class="post-title"><a class="a-post">Self-critical Sequence Training for Image Captioning with SPICE</a></p><p class="post-meta"><span class="date meta-item">Posted at&nbsp;2019-05-08</span></p><p class="post-abstract"></p><p><br></p>
<p>This is a project of interset and topic of this project is <strong>Self-critical Sequence Training for image captioning with SPICE</strong>. In this study, I was inspired by the paper <em>self-critical sequence training for image captioning</em> <span class="citation" data-cites="DBLP:journals/corr/RennieMMRG16">(Rennie et al. <a href="#ref-DBLP:journals/corr/RennieMMRG16" role="doc-biblioref">2016</a>)</span> (SCST) which is using REINFORCE algorithm with baseline to train a sequence generation model to do image captioning task.</p>
<p>I further investigated several evaluation metrics, which is not be used in the SCST, to be directly optimized. The metrics I studied are SPICE, LTEIC (learning to evaluating image captioning) and SPIDEr (linear combination of SPICE and SPIDEr, inspired by <span class="citation" data-cites="DBLP:journals/corr/LiuZYG016">(Liu et al. <a href="#ref-DBLP:journals/corr/LiuZYG016" role="doc-biblioref">2016</a>)</span>).</p>
<p>I found that directly optimizing SPICE will not lead us a better results overall mainly because the testing metrics I report on are mostly syntactic-based but SPICE is semantic-based. Optimizing SPIDEr shows a slightly better performance than optimizing CIDEr, which has a decent improvement on SPICE with relatively small trade-off on BLEU and CIDEr.</p>
<p><br></p>
<h3 id="introduction">Introduction</h3>
<p>Image caption has been a widely studied NLP task. However, there are still two challenges when using supervised learning. The first challenge is exposure bias, mentioned by <span class="citation" data-cites="ranzato2015sequence">(Ranzato et al. <a href="#ref-ranzato2015sequence" role="doc-biblioref">2015</a>)</span>. In detail, some text generation models are trained to predict next word given the previous ground truth word, but at test time, the prediction of next word will be made given a generated word instead of the previous ground truth word. As a result, the errors will be accumulated and I refer this as exposure bias since those models are never exposed to its predictions during training. Another is that most generative models are evaluated by non-differentiable NLP metrics such as BLEU <span class="citation" data-cites="papineni2002bleu">(Papineni et al. <a href="#ref-papineni2002bleu" role="doc-biblioref">2002</a>)</span>, ROUGE <span class="citation" data-cites="lin2004rouge">(Lin <a href="#ref-lin2004rouge" role="doc-biblioref">2004</a>)</span>, METEOR <span class="citation" data-cites="banerjee2005meteor">(Banerjee and Lavie <a href="#ref-banerjee2005meteor" role="doc-biblioref">2005</a>)</span>, CIDEr <span class="citation" data-cites="vedantam2015cider">(Vedantam, Lawrence Zitnick, and Parikh <a href="#ref-vedantam2015cider" role="doc-biblioref">2015</a>)</span> and SPICE <span class="citation" data-cites="anderson2016spice">(Anderson et al. <a href="#ref-anderson2016spice" role="doc-biblioref">2016</a>)</span>, however these models are trained by cross entropy which means the training is not directly optimizing the evaluation metrics.</p>
<p>For addressing these issues, <span class="citation" data-cites="ranzato2015sequence">(Ranzato et al. <a href="#ref-ranzato2015sequence" role="doc-biblioref">2015</a>)</span> firstly applied policy gradient to do image captioning and <span class="citation" data-cites="bahdanau2016actor">(Bahdanau et al. <a href="#ref-bahdanau2016actor" role="doc-biblioref">2016</a>)</span> employeed actor-critic <span class="citation" data-cites="konda2000actor">(Konda and Tsitsiklis <a href="#ref-konda2000actor" role="doc-biblioref">2000</a>)</span> algorithm. Afterwards, <span class="citation" data-cites="DBLP:journals/corr/RennieMMRG16">(Rennie et al. <a href="#ref-DBLP:journals/corr/RennieMMRG16" role="doc-biblioref">2016</a>)</span> and <span class="citation" data-cites="DBLP:journals/corr/LiuZYG016">(Liu et al. <a href="#ref-DBLP:journals/corr/LiuZYG016" role="doc-biblioref">2016</a>)</span> both use policy gradient methods to build sequence generation model.</p>
<p>Hence, I developed this project based on the paper <em>self-critical sequence training for image captioning</em>. And for reinforcement learning training, the reward fucntion design is very crusial and in self-critical paper, they used CIDEr as the reward metric. Therefore, our motiviation is to examinate varies of metrics not tested in SCST paper and hopefully I can make some improvement on the results.</p>
<p><br></p>
<h3 id="methods">Methods</h3>
<p>I keep kept the same models structure and REINFORCE algorithm which the self-critical sequence training paper used. Therefore, I could better focus on the reward metrics study. Due to the length limitation, some prerequisites knowledge can be found in the report which including following sections in detail:</p>
<ul>
<li>Captioning model</li>
<li>REINFORCE algorithm</li>
<li>REINFORCE algorithm with baseline</li>
</ul>
<p><br></p>
<h4 id="self-critical-sequence-training">self-critical sequence training</h4>
<p>Self-critical sequence training (SCST) is the key method of this project, proposed by <span class="citation" data-cites="DBLP:journals/corr/RennieMMRG16">(Rennie et al. <a href="#ref-DBLP:journals/corr/RennieMMRG16" role="doc-biblioref">2016</a>)</span>. The idea of SCST is using the current model under inference algorithm, i.e. greedy decoding, as a baseline for the REINFORCE algorithm. Therefore, the gradient can be written as following:</p>
<p><span class="math display">\[\begin{equation}
  \frac{\partial L(\theta)}{\partial s_{t}}\approx (r(w^s)-r(\hat{w}))(p_{\theta}(w_{t}|h_t)- 1_{w^s_{t}})
  \label{eq:GradientREINFORCE_b}
\end{equation}\]</span> <span class="math display">\[\begin{equation}
  \hat{w}_t = \arg \max_{w_t} p(w_t\,|\,h_t)
\end{equation}\]</span> where <span class="math inline">\(r(\hat{w})\)</span> is the reward obtained by the current model under inference mode, i.e. picking the word with maximum probablity instead of sampling word. Therefore if the sampled senetence obtain a higher reward <span class="math inline">\(r(w^s)\)</span> compared with greddy decoding reward <span class="math inline">\(r(\hat{w})\)</span>. The probabily of <span class="math inline">\(w^s\)</span> will be therefore increased. (See Figure 1 for details)</p>
<center>
<img src="https://i.postimg.cc/5xZ7Dwhk/SCST.png" width="100%" height="100%"> <br> <font size="3">Figure 1. Self-critical sequence training (SCST). The weight put on words of a sampled sentence from the model is deter- mined by the difference between the reward for the sampled sentence and the reward obtained by the estimated sentence under the test-time inference procedure (greedy inference depicted). This harmonizes learning with the inference procedure, and lowers the variance of the gradients, improving the training procedure.</font>
</center>
<p><br></p>
<h3 id="reward-metrics">Reward Metrics</h3>
<p><br></p>
<h4 id="semantic-propositional-image-caption-evaluation">Semantic propositional image caption evaluation</h4>
<p>SPICE <span class="citation" data-cites="anderson2016spice">(Anderson et al. <a href="#ref-anderson2016spice" role="doc-biblioref">2016</a>)</span> is a automatic image caption evalution metric which compares the semantic propositional content. Given a image <span class="math inline">\(I\)</span> with some references captions, usually generated by human, and a machine generated caption <span class="math inline">\(c\)</span>. The SPICE score, <span class="math inline">\(SPICE(c,S)\)</span>, indicate how good the machine-generated caption is.</p>
<p>Besides I encountered some issues with the default package. Details of SPICE package and my solution to its issues can be also found in the <a href="2019/05/07/SPICE/index.html">previous post</a>.</p>
<p><br></p>
<h4 id="learning-to-evaluate-image-captioning">Learning to evaluate image captioning</h4>
<p>So far, all evaluation mertics mentioned are rule-based automatic metric. Researchers <span class="citation" data-cites="cui2018learning">(Cui et al. <a href="#ref-cui2018learning" role="doc-biblioref">2018</a>)</span>, <span class="citation" data-cites="sharif-etal-2018-learning">(Sharif et al. <a href="#ref-sharif-etal-2018-learning" role="doc-biblioref">2018</a>)</span> start to get interested in learning-based evaluation metrics because learning-based methods not require expert-level linguistic knowledge but can be very powerful sometimes. Therefore, we choose the <em>Learning to evaluate image captioning</em> (LTEIC) to examinate how learning-based metric would performe as a reward metric.</p>
<p>It turns out the LTEIC model has a large bias, see my <a href="/2019/05/09/Discriminator/index.html">next post</a>, so result of LTEIC will not be reported later (but can be found in the next post also).</p>
<p><br></p>
<h3 id="experiments-and-results">Experiments and Results</h3>
<p><br></p>
<h4 id="dataset">Dataset</h4>
<p>I use the same dataset, MSCOCO dataset <span class="citation" data-cites="lin2014microsoft">(Lin et al. <a href="#ref-lin2014microsoft" role="doc-biblioref">2014</a>)</span>, and keep the same dataset split with the SCST paper. The training set contains 113,287 images and 5 reference captions each image, validation set contains 5k images and results are reported on a testing set contains 5k images. And our results will be reported on 5 widely used image caption evalution mertics, including BLEU4, METEOR, ROUGE-L, CIDEr and SPICE.</p>
<p><br></p>
<h4 id="experiments">Experiments</h4>
<p>At beginning, I hope directly optimizing SPICE could outperform the baseline, directly optimizing CIDEr, but it turns out directly optimizing SPICE might lead other metircs drop. A quick guess is, most of evaluation metrics I use is syntactic but SPICE is semantic based as figure below shows. Therefore, optimizing SPICE may not improve the syntactic quality, i.e. cause other scores droping.</p>
<center>
<img src="https://i.postimg.cc/8zYKpWjc/metrics.png" width="35%" height="35%"> <font size="3">Figure 2. evalution metrics overview</font>
</center>
<p><br></p>
<p>Inspired by <span class="citation" data-cites="DBLP:journals/corr/LiuZYG016">(Liu et al. <a href="#ref-DBLP:journals/corr/LiuZYG016" role="doc-biblioref">2016</a>)</span>, a linear combination of both CIDEr and SPICE might cover all aspects, lexical, syntactic and semantic. Hence, we further employeed the linear combination of CIDEr and SPICE (SPIDEr) as a reward metric. Hence, we report 5 sets experiments results, including CIDEr, SPICE, SPIDEr(.5), SPIDEr(.8) and LTEIC (where .5 and .8 is the weight of SPICE score).</p>
<p><br></p>
<h4 id="results">Results</h4>
<p>5 sets experiments are trained with 40 epoches each and evaluate the model on test set every 6000 iterations. The last evaluation results on test set are reported in table below:</p>
<p><font size="3">Table 1. experiment results, <strong>bold font</strong> indicate the best result in all experiments, <br> <strong>underline</strong> indicate outperforming the baseline</font></p>
<table>
<thead>
<tr class="header">
<th>Experiments</th>
<th style="text-align: center;">BLEU4</th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;">ROUGE-L</th>
<th style="text-align: center;">CIDEr</th>
<th style="text-align: center;">SPICE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>CIDEr(baseline)</td>
<td style="text-align: center;"><strong>0.323</strong></td>
<td style="text-align: center;">0.256</td>
<td style="text-align: center;">0.544</td>
<td style="text-align: center;"><strong>1.063</strong></td>
<td style="text-align: center;">0.188</td>
</tr>
<tr class="even">
<td>SPICE</td>
<td style="text-align: center;">0.186</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.602</td>
<td style="text-align: center;"><strong>0.241</strong></td>
</tr>
<tr class="odd">
<td>SPIDEr(.5)</td>
<td style="text-align: center;">0.321</td>
<td style="text-align: center;"><strong>0.258</strong></td>
<td style="text-align: center;"><strong><strong>0.545</strong></strong></td>
<td style="text-align: center;">1.058</td>
<td style="text-align: center;"><strong>0.196</strong></td>
</tr>
<tr class="even">
<td>SPIDEr(.8)</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;"><strong><strong>0.261</strong></strong></td>
<td style="text-align: center;">0.541</td>
<td style="text-align: center;">1.040</td>
<td style="text-align: center;"><strong>0.207</strong></td>
</tr>
</tbody>
</table>
<p><br> In general, directly optimzing SPICE might cause a worse results overall as we previously discussed but end with a very high SPICE score. Optimzing LTEIC does not work at all as we expected. Optimzing CIDEr, SPIDEr(.5) and SPIDEr(.8) seems have very close results, no one is dominating others. Therefore we compared these 3 experiments, taking optimizing CIDEr as baseline, in figure below. It shows optimizing SPIDEr has a little sacrifices on BLEU4 and CIDEr but will lead a decent improvement on SPICE (and small improvement on METEOR).</p>
<center>
<img src="https://i.postimg.cc/xTtpZBQK/comparison.png" width="70%" height="70%"> <font size="3">Figure 3. improvement% compared with baseline (optmizing CIDEr)</font>
</center>
<p><br> Besides, example captions from different models on 10 randomly selected COCO images can be found in table below.</p>
<p><font size="3">Table 2. ungrammatically captions when directly optimizing SPICE</font></p>
<table>
<colgroup>
<col style="width: 14%">
<col style="width: 42%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th>Image</th>
<th style="text-align: center;">Ground Truth Captions</th>
<th style="text-align: center;">Generated Captions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><img src="https://i.postimg.cc/G3P5FHSg/COCO-val2014-000000057363.jpg" width="80%" height="80%"></td>
<td style="text-align: center;">a red and yellow fire truck and some buildings<br>An overhead view shows a fire engine in the street.<br>A red and yellow fire truck with ladders on top<br>A firetruck is parked in the street in between stop lights.<br>A fire truck (ladder truck) drives down a street in the city.</td>
<td style="text-align: center;">CIDEr: a red fire truck parked on the side of a street<br>SPICE: a red fire truck parked on a city street together a building together a man<br>SPIDEr(.5): a red fire truck parked on the side of a street<br>SPIDEr(.8): a red fire truck parked in front of a street</td>
</tr>
<tr class="even">
<td><img src="https://i.postimg.cc/028HcPTk/COCO-val2014-000000541924.jpg" width="80%" height="80%"></td>
<td style="text-align: center;">A woman walking on a city street in a red coat.<br>A group of people that are standing on the side of a street.<br>A woman in a red jacket crossing the street<br>a street light some people and a woman wearing a red jacket<br>A blonde woman in a red coat crosses the street with her friend.</td>
<td style="text-align: center;">CIDEr: a woman walking down a street with a traffic light<br>SPICE: a group of people and a woman walking down a city street with a traffic light<br>SPIDEr(.5): a group of people walking down a street<br>SPIDEr(.8): a group of people walking down a city street</td>
</tr>
</tbody>
</table>
<p><br></p>
<h3 id="conclusion">Conclusion</h3>
<p>Directly optimizing SPICE will not lead us a better results overall mainly because the testing metrics we report on are mostly syntactic-based but SPICE is semantic-based. And LTEIC itself has a very strong bias (see <a href="/2019/05/09/Discriminator/index.html">next post</a> for details), therefore it doesn't work and indeed we didn't expect it to work.</p>
<p>Optimizing SPIDEr shows a slightly better performance than optimizing CIDEr, which has a decent improvement on SPICE with relatively small trade-off on BLEU and CIDEr.</p>
<p><br></p>
<h3 id="acknowledgments">Acknowledgments</h3>
<p>I acknowledgment Dr. Natalie Parde for advices on this project. Thanks Ruotian Luo for high-quality implementation of <a href="https://github.com/ruotianluo/self-critical.pytorch" target="_blank" rel="noopener"><em>self-critical sequence training</em></a>. Thanks Yin Cui for releasing their code for <a href="https://github.com/richardaecn/cvpr18-caption-eval" target="_blank" rel="noopener"><em>learning to evaluate image captioning</em></a> And I would like to thank Dr. Natalie Parde, Dr. Siqi Liu for advices on improving SPICE time efficiency.</p>
<p><br></p>
<h3 id="bibliography" class="unnumbered">Bibliography</h3>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-anderson2016spice">
<p>Anderson, Peter, Basura Fernando, Mark Johnson, and Stephen Gould. 2016. “Spice: Semantic Propositional Image Caption Evaluation.” In <em>European Conference on Computer Vision</em>, 382–98. Springer.</p>
</div>
<div id="ref-bahdanau2016actor">
<p>Bahdanau, Dzmitry, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. 2016. “An Actor-Critic Algorithm for Sequence Prediction.” <em>arXiv Preprint arXiv:1607.07086</em>.</p>
</div>
<div id="ref-banerjee2005meteor">
<p>Banerjee, Satanjeev, and Alon Lavie. 2005. “METEOR: An Automatic Metric for Mt Evaluation with Improved Correlation with Human Judgments.” In <em>Proceedings of the Acl Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</em>, 65–72.</p>
</div>
<div id="ref-cui2018learning">
<p>Cui, Yin, Guandao Yang, Andreas Veit, Xun Huang, and Serge Belongie. 2018. “Learning to Evaluate Image Captioning.” In <em>Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition</em>, 5804–12.</p>
</div>
<div id="ref-konda2000actor">
<p>Konda, Vijay R, and John N Tsitsiklis. 2000. “Actor-Critic Algorithms.” In <em>Advances in Neural Information Processing Systems</em>, 1008–14.</p>
</div>
<div id="ref-lin2004rouge">
<p>Lin, Chin-Yew. 2004. “Rouge: A Package for Automatic Evaluation of Summaries.” <em>Text Summarization Branches Out</em>.</p>
</div>
<div id="ref-lin2014microsoft">
<p>Lin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. “Microsoft Coco: Common Objects in Context.” In <em>European Conference on Computer Vision</em>, 740–55. Springer.</p>
</div>
<div id="ref-DBLP:journals/corr/LiuZYG016">
<p>Liu, Siqi, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy. 2016. “Optimization of Image Description Metrics Using Policy Gradient Methods.” <em>CoRR</em> abs/1612.00370. <a href="http://arxiv.org/abs/1612.00370" target="_blank" rel="noopener">http://arxiv.org/abs/1612.00370</a>.</p>
</div>
<div id="ref-papineni2002bleu">
<p>Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. “BLEU: A Method for Automatic Evaluation of Machine Translation.” In <em>Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</em>, 311–18. Association for Computational Linguistics.</p>
</div>
<div id="ref-ranzato2015sequence">
<p>Ranzato, Marc’Aurelio, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2015. “Sequence Level Training with Recurrent Neural Networks.” <em>arXiv Preprint arXiv:1511.06732</em>.</p>
</div>
<div id="ref-DBLP:journals/corr/RennieMMRG16">
<p>Rennie, Steven J., Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava Goel. 2016. “Self-Critical Sequence Training for Image Captioning.” <em>CoRR</em> abs/1612.00563. <a href="http://arxiv.org/abs/1612.00563" target="_blank" rel="noopener">http://arxiv.org/abs/1612.00563</a>.</p>
</div>
<div id="ref-sharif-etal-2018-learning">
<p>Sharif, Naeha, Lyndon White, Mohammed Bennamoun, and Syed Afaq Ali Shah. 2018. “Learning-Based Composite Metrics for Improved Caption Evaluation.” In <em>Proceedings of ACL 2018, Student Research Workshop</em>, 14–20. Melbourne, Australia: Association for Computational Linguistics. <a href="https://www.aclweb.org/anthology/P18-3003" target="_blank" rel="noopener">https://www.aclweb.org/anthology/P18-3003</a>.</p>
</div>
<div id="ref-vedantam2015cider">
<p>Vedantam, Ramakrishna, C Lawrence Zitnick, and Devi Parikh. 2015. “Cider: Consensus-Based Image Description Evaluation.” In <em>Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition</em>, 4566–75.</p>
</div>
</div>
<p></p></div><div class="share"><span>Share</span>&nbsp;<span class="soc"><a href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank" class="fa fa-bookmark"></a></span><span class="soc"><a href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));" class="fa fa-weibo"></a></span><span class="soc"><a href="http://twitter.com/home?status=http://www.zishun.xyz/Blog/2019/05/08/Self-critical/%20Zishun's Blog%20Self-critical Sequence Training for Image Captioning with SPICE" class="fa fa-twitter"></a></span></div><div class="pagination"><p class="clearfix"><span class="pre pagbuttons"><a role="navigation" href="/Blog/2019/05/09/Discriminator/" title="Optimizing Learning-Based Metric for Image Captioning"><i class="fa fa-angle-double-left"></i>&nbsp;Previous post: Optimizing Learning-Based Metric for Image Captioning</a></span><span>&nbsp;</span><span class="next pagbuttons"><a role="navigation" href="/Blog/2019/05/07/SPICE/" title="Issues with SPICE Package and Solution">Next post: Issues with SPICE Package and Solution&nbsp;<i class="fa fa-angle-double-right"></i></a></span></p></div></div></div></div><div class="visible-xs site-bottom-footer"><footer><p>&copy;&nbsp;2019&nbsp;<a target="_blank" href="http://www.zishun.xyz" rel="noopener noreferrer">Zishun Yu</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div></div><script src="/js/jquery-3.1.0.min.js"></script><script src="/js/bootstrap.min.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/google-analytics.js"></script><script src="/js/typography.js"></script></body></html>