<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Zishun Yu"><title>Optimizing Learning-Based Metric for Image Captioning · Zishun's Blog</title><meta name="description" content="This is an additional work to previous post, SCST with SPICE. I was not only optimizing automatic evaluation mertics like CIDEr, SPIDEr which are all "><meta name="keywords" content><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/bootstrap.min.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/style.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML" async></script><link rel="stylesheet" href="/css/prism.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div id="stage" class="container"><div class="row"><div id="side-bar" class="col-sm-3 col-xs-12 side-container invisible"><div class="vertical-text site-title"><h3 tabindex="-1" class="site-title-small"><a href="/" class="a-title">Perception</a></h3><h1 tabindex="-1" class="site-title-large"><a href="/" class="a-title">認識能力</a></h1><!--h6(onclick="triggerSiteNav()") Trigger--></div><br class="visible-lg visible-md visible-sm"><div id="site-nav" class="site-title-links"><ul><li><a href="/">Home</a></li><li></li><li><a href="/index.html"></a></li><li><a href="/404.html"></a></li><li><a href="/Blog/index.html" class="current">Blog</a></li><li><a href="/CV/index.html">CV</a></li><li class="soc"><a href="https://github.com/ZishunYu" target="_blank" rel="noopener noreferrer"><i class="fa fa-github">&nbsp;</i></a></li></ul><div class="visible-lg visible-md visible-sm site-nav-footer"><br class="site-nav-footer-br"><footer><p>&copy;&nbsp;2019&nbsp;<a target="_blank" href="http://www.zishun.xyz" rel="noopener noreferrer">Zishun Yu</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div><div id="main-container" class="col-sm-9 col-xs-12 main-container invisible"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post-container"><p class="post-title"><a class="a-post">Optimizing Learning-Based Metric for Image Captioning</a></p><p class="post-meta"><span class="date meta-item">Posted at&nbsp;2019-05-09</span></p><p class="post-abstract"></p><p><br></p>
<p>This is an additional work to <a href="/2019/05/08/Self-critical/index.html">previous post</a>, SCST with SPICE. I was not only optimizing automatic evaluation mertics like CIDEr, SPIDEr which are all rule-based metrics, but also want to try some learning-based methods. Researchers [@cui2018learning], [@sharif-etal-2018-learning] start to get interested in learning-based evaluation metrics because learning-based methods not require expert-level linguistic knowledge but can be very powerful sometimes. Therefore, I choose the <em>Learning to evaluate image captioning</em> (LTEIC) to examinate how learning-based metric would performe as a reward metric.</p>
<p><br></p>
<h3 id="Learning-to-evaluate-image-captioning"><a href="#Learning-to-evaluate-image-captioning" class="headerlink" title="Learning to evaluate image captioning"></a>Learning to evaluate image captioning</h3><p>Thanks Yin Cui for releasing their <a href="https://github.com/richardaecn/cvpr18-caption-eval" target="_blank" rel="noopener">source code</a></p>
<p><br></p>
<h4 id="model-overview"><a href="#model-overview" class="headerlink" title="model overview"></a>model overview</h4><p>The intuition of LTEIC is to build a model to distinguish between machine-generated and human-written captions. Therefore we could build a discriminator accepting image, candidate caption, groud truth caption(s) and return a score of how likely the candidate is human-written. Hence, the LTEIC model have the arichiture as figure below shown:</p>
<center><br>    <img src="https://i.postimg.cc/htvZRM4N/LTEIC-arichitecture.png" width="80%" height="80%"><br>    <font size="3">Figure 1. LTEIC model architecture</font><br></center>

<p><br><br>It embeds the input image with a ResNet101 and embeds the reference captions with a LSTM. Image feature and reference captions representation are concatenated as the context vector. The candidate caption will also be fed into the same LSTM. The context representation and the candidate caption representation will be combine by compact bilinear pooling. Afterwards, a softmax classifier will score, as below, the combined feature from 0 to 1, the higher the better.</p>
<p>\begin{equation}<br>  score_\theta(\hat{c}, C(I, S)) = P(\hat{c} \ is \ human \ written|C(I, S), \theta)<br>\end{equation}</p>
<p>where $I$ is the image, $S$ is the reference captions, $C(I,S)$ denote the context, $\hat{c}$ is the candidate caption and $\theta$ is the network parameters.</p>
<p><br></p>
<h4 id="Model-performance"><a href="#Model-performance" class="headerlink" title="Model performance"></a>Model performance</h4><p>I trained the discriminator using <em>show attend and tell</em> [@xu2015show] model, and tested our discriminator on <em>show attend and tell</em> itself, <em>neural talk</em> [@karpathy2015deep], <em>show and tell</em> [@vinyals2015show] and human captions. Results are shown below:<br><br></p>
<font size="3">Table 4. discriminator performance on test set</font>

<table>
<thead>
<tr>
<th></th>
<th style="text-align:center">show attend and tell</th>
<th style="text-align:center">neural talk</th>
<th style="text-align:center">show and tell</th>
<th style="text-align:center">human</th>
</tr>
</thead>
<tbody>
<tr>
<td>LTEIC score</td>
<td style="text-align:center">0.074</td>
<td style="text-align:center">0.342</td>
<td style="text-align:center">0.408</td>
<td style="text-align:center">0.797</td>
</tr>
<tr>
<td>Accuracy</td>
<td style="text-align:center">0.968</td>
<td style="text-align:center">0.651</td>
<td style="text-align:center">0.564</td>
<td style="text-align:center">0.886</td>
</tr>
</tbody>
</table>
<p><br><br>This discriminator shows very accurate results on <em>show attend and tell</em> model and human captions but it struggles on captions generated by <em>neural talk</em> and <em>show and tell</em>. Which indicate a large bias, so I don’t really expect it working as a reward metric (but I still use this frozen model as a scorer later even though it has a large bias.)</p>
<p><br></p>
<h4 id="Some-thoughts"><a href="#Some-thoughts" class="headerlink" title="Some thoughts"></a>Some thoughts</h4><p>It turns out the idea of using a discriminator as reward metrics is very close to the idea of Generative Adversarial Networks(GAN) [@goodfellow2014generative]. But due to the time limit, I didn’t try it but I believe it would be a good idea to improve our generative model and there were a few works [@dai2017towards], [@liang2017recurrent] using GAN emerged. Therefore, using GAN would be a better choice instead of using a frozen pretained discriminator.</p>
<p><br></p>
<h3 id="LTEIC-as-a-reward-metric"><a href="#LTEIC-as-a-reward-metric" class="headerlink" title="LTEIC as a reward metric"></a>LTEIC as a reward metric</h3><p>%TODO</p>
<p><br></p>
<h3 id="Bibliography"><a href="#Bibliography" class="headerlink" title="Bibliography"></a>Bibliography</h3><p></p></div><div class="share"><span>Share</span>&nbsp;<span class="soc"><a href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank" class="fa fa-bookmark"></a></span><span class="soc"><a href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));" class="fa fa-weibo"></a></span><span class="soc"><a href="http://twitter.com/home?status=http://www.zishun.xyz/Blog/2019/05/09/Discriminator/%20Zishun's Blog%20Optimizing Learning-Based Metric for Image Captioning" class="fa fa-twitter"></a></span></div><div class="pagination"><p class="clearfix"><span>&nbsp;</span><span class="next pagbuttons"><a role="navigation" href="/Blog/2019/05/08/Self-critical/" title="Self-critical Sequence Training for Image Captioning with SPICE">Next post: Self-critical Sequence Training for Image Captioning with SPICE&nbsp;<i class="fa fa-angle-double-right"></i></a></span></p></div></div></div></div><div class="visible-xs site-bottom-footer"><footer><p>&copy;&nbsp;2019&nbsp;<a target="_blank" href="http://www.zishun.xyz" rel="noopener noreferrer">Zishun Yu</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div></div><script src="/js/jquery-3.1.0.min.js"></script><script src="/js/bootstrap.min.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/google-analytics.js"></script><script src="/js/typography.js"></script></body></html>