<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Zishun Yu"><title>Optimizing Learning-Based Metric for Image Captioning · Zishun's Blog</title><meta name="description" content="This is an additional work to previous post, SCST with SPICE. I was not only optimizing automatic evaluation mertics like CIDEr, SPIDEr which are all "><meta name="keywords" content><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/bootstrap.min.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/style.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML" async></script><link rel="stylesheet" href="/css/prism.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div id="stage" class="container"><div class="row"><div id="side-bar" class="col-sm-3 col-xs-12 side-container invisible"><div class="vertical-text site-title"><h3 tabindex="-1" class="site-title-small"><a href="/" class="a-title">Perception</a></h3><h1 tabindex="-1" class="site-title-large"><a href="/" class="a-title">認識能力</a></h1><!--h6(onclick="triggerSiteNav()") Trigger--></div><br class="visible-lg visible-md visible-sm"><div id="site-nav" class="site-title-links"><ul><li><a href="/">Home</a></li><li></li><li><a href="/index.html"></a></li><li><a href="/CV/index.html">CV</a></li><li><a href="/Blog/index.html" class="current">Blog</a></li><li class="soc"><a href="https://github.com/ZishunYu" target="_blank" rel="noopener noreferrer"><i class="fa fa-github">&nbsp;</i></a></li></ul><div class="visible-lg visible-md visible-sm site-nav-footer"><br class="site-nav-footer-br"><footer><p>&copy;&nbsp;2019&nbsp;<a target="_blank" href="http://www.zishun.xyz" rel="noopener noreferrer">Zishun Yu</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div><div id="main-container" class="col-sm-9 col-xs-12 main-container invisible"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post-container"><p class="post-title"><a>Optimizing Learning-Based Metric for Image Captioning</a></p><p class="post-meta"><span class="date meta-item">Posted at&nbsp;2019-05-09</span></p><p class="post-abstract"></p><p><br></p>
<p>This is an additional work to <a href="/2019/05/08/Self-critical/index.html">previous post</a>, SCST with SPICE. I was not only optimizing automatic evaluation mertics like CIDEr, SPIDEr which are all rule-based metrics, but also want to try some learning-based methods. Researchers <span class="citation" data-cites="cui2018learning">(Cui et al. <a href="#ref-cui2018learning" role="doc-biblioref">2018</a>)</span>, <span class="citation" data-cites="sharif-etal-2018-learning">(Sharif et al. <a href="#ref-sharif-etal-2018-learning" role="doc-biblioref">2018</a>)</span> start to get interested in learning-based evaluation metrics because learning-based methods not require expert-level linguistic knowledge but can be very powerful sometimes. Therefore, I choose the <em>Learning to evaluate image captioning</em> (LTEIC) to examinate how learning-based metric would performe as a reward metric.</p>
<p><br></p>
<h3 id="learning-to-evaluate-image-captioning">Learning to evaluate image captioning</h3>
<p>Thanks Yin Cui for releasing their <a href="https://github.com/richardaecn/cvpr18-caption-eval" target="_blank" rel="noopener">source code</a></p>
<p><br></p>
<h4 id="model-overview">model overview</h4>
<p>The intuition of LTEIC is to build a model to distinguish between machine-generated and human-written captions. Therefore we could build a discriminator accepting image, candidate caption, groud truth caption(s) and return a score of how likely the candidate is human-written. Hence, the LTEIC model have the arichiture as figure below shown:</p>
<center>
<img src="https://i.postimg.cc/htvZRM4N/LTEIC-arichitecture.png" width="80%" height="80%"> <font size="3">Figure 1. LTEIC model architecture</font>
</center>
<p><br> It embeds the input image with a ResNet101 and embeds the reference captions with a LSTM. Image feature and reference captions representation are concatenated as the context vector. The candidate caption will also be fed into the same LSTM. The context representation and the candidate caption representation will be combine by compact bilinear pooling. Afterwards, a softmax classifier will score, as below, the combined feature from 0 to 1, the higher the better.</p>
<p><span class="math display">\[\begin{equation}
  score_\theta(\hat{c}, C(I, S)) = P(\hat{c} \ is \ human \ written|C(I, S), \theta)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(I\)</span> is the image, <span class="math inline">\(S\)</span> is the reference captions, <span class="math inline">\(C(I,S)\)</span> denote the context, <span class="math inline">\(\hat{c}\)</span> is the candidate caption and <span class="math inline">\(\theta\)</span> is the network parameters.</p>
<p><br></p>
<h4 id="model-performance">Model performance</h4>
<p>I trained the discriminator using <em>show attend and tell</em> <span class="citation" data-cites="xu2015show">(Xu et al. <a href="#ref-xu2015show" role="doc-biblioref">2015</a>)</span> model, and tested our discriminator on <em>show attend and tell</em> itself, <em>neural talk</em> <span class="citation" data-cites="karpathy2015deep">(Karpathy and Fei-Fei <a href="#ref-karpathy2015deep" role="doc-biblioref">2015</a>)</span>, <em>show and tell</em> <span class="citation" data-cites="vinyals2015show">(Vinyals et al. <a href="#ref-vinyals2015show" role="doc-biblioref">2015</a>)</span> and human captions. Results are shown below: <br></p>
<p><font size="3">Table 4. discriminator performance on test set</font></p>
<table>
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;">show attend and tell</th>
<th style="text-align: center;">neural talk</th>
<th style="text-align: center;">show and tell</th>
<th style="text-align: center;">human</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>LTEIC score</td>
<td style="text-align: center;">0.074</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.797</td>
</tr>
<tr class="even">
<td>Accuracy</td>
<td style="text-align: center;">0.968</td>
<td style="text-align: center;">0.651</td>
<td style="text-align: center;">0.564</td>
<td style="text-align: center;">0.886</td>
</tr>
</tbody>
</table>
<p><br> This discriminator shows very accurate results on <em>show attend and tell</em> model and human captions but it struggles on captions generated by <em>neural talk</em> and <em>show and tell</em>. Which indicate a large bias, so I don’t really expect it working as a reward metric (but I still use this frozen model as a scorer later even though it has a large bias.)</p>
<p><br></p>
<h4 id="some-thoughts">Some thoughts</h4>
<p>It turns out the idea of using a discriminator as reward metrics is very close to the idea of Generative Adversarial Networks(GAN) <span class="citation" data-cites="goodfellow2014generative">(Goodfellow et al. <a href="#ref-goodfellow2014generative" role="doc-biblioref">2014</a>)</span>. But due to the time limit, I didn’t try it but I believe it would be a good idea to improve our generative model and there were a few works <span class="citation" data-cites="dai2017towards">(Dai et al. <a href="#ref-dai2017towards" role="doc-biblioref">2017</a>)</span>, <span class="citation" data-cites="liang2017recurrent">(Liang et al. <a href="#ref-liang2017recurrent" role="doc-biblioref">2017</a>)</span> using GAN emerged. Therefore, using GAN would be a better choice instead of using a frozen pretained discriminator.</p>
<p><br></p>
<h3 id="lteic-as-a-reward-metric">LTEIC as a reward metric</h3>
<p>%TODO</p>
<p><br></p>
<h3 id="bibliography" class="unnumbered">Bibliography</h3>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-cui2018learning">
<p>Cui, Yin, Guandao Yang, Andreas Veit, Xun Huang, and Serge Belongie. 2018. “Learning to Evaluate Image Captioning.” In <em>Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition</em>, 5804–12.</p>
</div>
<div id="ref-dai2017towards">
<p>Dai, Bo, Sanja Fidler, Raquel Urtasun, and Dahua Lin. 2017. “Towards Diverse and Natural Image Descriptions via a Conditional Gan.” In <em>Proceedings of the Ieee International Conference on Computer Vision</em>, 2970–9.</p>
</div>
<div id="ref-goodfellow2014generative">
<p>Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. “Generative Adversarial Nets.” In <em>Advances in Neural Information Processing Systems</em>, 2672–80.</p>
</div>
<div id="ref-karpathy2015deep">
<p>Karpathy, Andrej, and Li Fei-Fei. 2015. “Deep Visual-Semantic Alignments for Generating Image Descriptions.” In <em>Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition</em>, 3128–37.</p>
</div>
<div id="ref-liang2017recurrent">
<p>Liang, Xiaodan, Zhiting Hu, Hao Zhang, Chuang Gan, and Eric P Xing. 2017. “Recurrent Topic-Transition Gan for Visual Paragraph Generation.” In <em>Proceedings of the Ieee International Conference on Computer Vision</em>, 3362–71.</p>
</div>
<div id="ref-sharif-etal-2018-learning">
<p>Sharif, Naeha, Lyndon White, Mohammed Bennamoun, and Syed Afaq Ali Shah. 2018. “Learning-Based Composite Metrics for Improved Caption Evaluation.” In <em>Proceedings of ACL 2018, Student Research Workshop</em>, 14–20. Melbourne, Australia: Association for Computational Linguistics. <a href="https://www.aclweb.org/anthology/P18-3003" target="_blank" rel="noopener">https://www.aclweb.org/anthology/P18-3003</a>.</p>
</div>
<div id="ref-vinyals2015show">
<p>Vinyals, Oriol, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. “Show and Tell: A Neural Image Caption Generator.” In <em>Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition</em>, 3156–64.</p>
</div>
<div id="ref-xu2015show">
<p>Xu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.” In <em>International Conference on Machine Learning</em>, 2048–57.</p>
</div>
</div>
<p></p></div><div class="share"><span>Share</span>&nbsp;<span class="soc"><a href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank" class="fa fa-bookmark"></a></span><span class="soc"><a href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));" class="fa fa-weibo"></a></span><span class="soc"><a href="http://twitter.com/home?status=http://www.zishun.xyz/Blog/2019/05/09/Discriminator/%20Zishun's Blog%20Optimizing Learning-Based Metric for Image Captioning" class="fa fa-twitter"></a></span></div><div class="pagination"><p class="clearfix"><span>&nbsp;</span><span class="next pagbuttons"><a role="navigation" href="/Blog/2019/05/08/Self-critical/" title="Self-critical Sequence Training for Image Captioning with SPICE">Next post: Self-critical Sequence Training for Image Captioning with SPICE&nbsp;<i class="fa fa-angle-double-right"></i></a></span></p></div></div></div></div><div class="visible-xs site-bottom-footer"><footer><p>&copy;&nbsp;2019&nbsp;<a target="_blank" href="http://www.zishun.xyz" rel="noopener noreferrer">Zishun Yu</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div></div><script src="/js/jquery-3.1.0.min.js"></script><script src="/js/bootstrap.min.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/google-analytics.js"></script><script src="/js/typography.js"></script></body></html>